{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZjzwKNkuaSW"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://framerusercontent.com/images/9vH8BcjXKRcC5OrSfkohhSyDgX0.png\" width=\"130\">\n",
        "<img src=\"https://debuggercafe.com/wp-content/uploads/2025/01/smolagents-logo.png\" width=\"140\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>SUTRA by TWO Platforms</h2>\n",
        "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRA‚Äôs dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
        "\n",
        "  <h2>Smolagents With SUTRA</h2>\n",
        "  <p>Smolagents is an AI agent framework recently launched by the Hugging Face team to simplify the process of developing AI agents.\n",
        "\n",
        "It‚Äôs a lightweight library that prioritizes practicality. This means it can build AI agents in a few lines of code but focuses more on simple implementation than creating the whole agent system in production.</p>\n",
        "</div>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1trbomQyteYsEjuH-ejyTtZqqk2JQtXzA?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAlmORCywBAl"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fooO7HybwNZz"
      },
      "source": [
        "###üîß 1. Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ALsRvMBGmCiV"
      },
      "outputs": [],
      "source": [
        "!pip install duckduckgo-search pyttsx3 gTTS langchain_community langchain_chroma langchain_huggingface \"smolagents[litellm]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYGgPC9QwRNs"
      },
      "source": [
        "###üîë 2. Set Environment Variables (API Keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP5S-0rHnWWV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIAuIgmCwris"
      },
      "source": [
        "###Initialize Sutra LLM via LiteLLMModel (SmolAgents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du3gNOSzmOo-",
        "outputId": "ea2446eb-a20f-4773-bf36-bb82ed329a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatMessage(role='assistant', content='‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π, ‡§ú‡§ø‡§∏‡•á ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§Æ‡•á‡§Ç Mars ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§∏‡•å‡§∞ ‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡§æ ‡§ö‡•å‡§•‡§æ ‡§ó‡•ç‡§∞‡§π ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡•á ‡§≤‡§æ‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§ú‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§∞‡•ã‡§Æ‡§® ‡§Ø‡•Å‡§¶‡•ç‡§ß ‡§ï‡•á ‡§¶‡•á‡§µ‡§§‡§æ ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§™‡§∞ ‡§∞‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡§æ ‡§∞‡§Ç‡§ó ‡§≤‡§æ‡§≤ ‡§π‡•à, ‡§ú‡•ã ‡§ï‡§ø ‡§á‡§∏‡§ï‡•á ‡§∏‡§§‡§π ‡§™‡§∞ ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§Ü‡§Ø‡§∞‡§® ‡§ë‡§ï‡•ç‡§∏‡§æ‡§á‡§° (‡§ú‡§Ç‡§ó) ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ ‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§∏ ‡§≤‡§ó‡§≠‡§ó 6,779 ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§π‡•à, ‡§ú‡•ã ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡•á ‡§Ü‡§ß‡•á ‡§∏‡•á ‡§•‡•ã‡§°‡§º‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à‡•§ ‡§Ø‡§π ‡§ó‡•ç‡§∞‡§π ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§∏‡•á ‡§§‡•Ä‡§∏‡§∞‡•Ä ‡§¶‡•Ç‡§∞‡•Ä ‡§™‡§∞ ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§ï‡•Ä ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ã ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§≤‡§ó‡§≠‡§ó 687 ‡§¶‡§ø‡§® ‡§≤‡§ó‡§§‡•á ‡§π‡•à‡§Ç‡•§\\n\\n‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•Ä ‡§∏‡§§‡§π ‡§™‡§∞ ‡§ï‡§à ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§è‡§Å ‡§π‡•à‡§Ç, ‡§ú‡•à‡§∏‡•á ‡§ï‡§ø ‡§ó‡§π‡§∞‡•Ä ‡§ò‡§æ‡§ü‡§ø‡§Ø‡§æ‡§Å, ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ‡§Æ‡•Å‡§ñ‡•Ä ‡§î‡§∞ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§Æ‡•à‡§¶‡§æ‡§®‡•Ä ‡§á‡§≤‡§æ‡§ï‡§º‡•á‡•§ ‡§á‡§∏‡§ï‡•Ä ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ ‡§ì‡§≤‡§Ç‡§™‡§∏ ‡§Æ‡•â‡§®‡•ç‡§∏ ‡§π‡•à, ‡§ú‡•ã ‡§∏‡•å‡§∞ ‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡§æ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ‡§Æ‡•Å‡§ñ‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§ï‡•Ä ‡§ä‡§Å‡§ö‡§æ‡§à ‡§≤‡§ó‡§≠‡§ó 22 ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ, ‡§µ‡•á‡§≤‡§ø‡§∏ ‡§Æ‡§æ‡§∞‡§ø‡§®‡•á‡§∞‡§ø‡§∏, ‡§ú‡•ã ‡§è‡§ï ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ò‡§æ‡§ü‡•Ä ‡§π‡•à, ‡§≤‡§ó‡§≠‡§ó 4,000 ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§≤‡§Ç‡§¨‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π ‡§ß‡§∞‡§§‡•Ä ‡§ï‡•Ä ‡§ó‡•ç‡§∞‡•à‡§Ç‡§° ‡§ï‡•à‡§®‡•ç‡§Ø‡§® ‡§∏‡•á ‡§≠‡•Ä ‡§¨‡§°‡§º‡•Ä ‡§π‡•à‡•§ ‡§á‡§® ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§ì‡§Ç ‡§®‡•á ‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•ã ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§ï‡§æ ‡§è‡§ï ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§µ‡§ø‡§∑‡§Ø ‡§¨‡§®‡§æ ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§\\n\\n‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡•á ‡§µ‡§æ‡§Ø‡•Å‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡•Ä ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡•Ä ‡§§‡•Å‡§≤‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§™‡§§‡§≤‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§§‡§É ‡§ï‡§æ‡§∞‡•ç‡§¨‡§® ‡§°‡§æ‡§á‡§ë‡§ï‡•ç‡§∏‡§æ‡§á‡§° (95.3%), ‡§®‡§æ‡§á‡§ü‡•ç‡§∞‡•ã‡§ú‡§® (2.7%) ‡§î‡§∞ ‡§Ü‡§∞‡•ç‡§ó‡§® (1.6%) ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à‡§Ç‡•§ ‡§µ‡§æ‡§Ø‡•Å‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡•Ä ‡§á‡§∏ ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞ ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§¨‡§π‡•Å‡§§ ‡§ï‡§Æ ‡§∞‡§π‡§§‡§æ ‡§π‡•à, ‡§î‡§∏‡§§‡§® -80 ‡§°‡§ø‡§ó‡•ç‡§∞‡•Ä ‡§´‡§æ‡§∞‡•á‡§®‡§π‡§æ‡§á‡§ü (-62 ‡§°‡§ø‡§ó‡•ç‡§∞‡•Ä ‡§∏‡•á‡§≤‡•ç‡§∏‡§ø‡§Ø‡§∏) ‡§§‡§ï ‡§ó‡§ø‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ, ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞ ‡§ß‡•Ç‡§≤ ‡§≠‡§∞‡•Ä ‡§Ü‡§Å‡§ß‡§ø‡§Ø‡§æ‡§Å ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç, ‡§ú‡•ã ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§™‡•Ç‡§∞‡•á ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§ï‡•ã ‡§¢‡§ï ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡§Ç‡•§\\n\\n‡§™‡§ø‡§õ‡§≤‡•á ‡§ï‡•Å‡§õ ‡§µ‡§∞‡•ç‡§∑‡•ã‡§Ç ‡§Æ‡•á‡§Ç, ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞ ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•Ä ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§ñ‡•ã‡§ú ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§à ‡§Æ‡§ø‡§∂‡§® ‡§≠‡•á‡§ú‡•á ‡§ó‡§è ‡§π‡•à‡§Ç‡•§ NASA ‡§ï‡•á ‡§∞‡•ã‡§µ‡§∞ ‡§ú‡•à‡§∏‡•á ‡§ï‡•ç‡§Ø‡•Ç‡§∞‡§ø‡§Ø‡•ã‡§∏‡§ø‡§ü‡•Ä ‡§î‡§∞ ‡§™‡§∞‡•ç‡§∏‡•á‡§µ‡•á‡§∞‡•á‡§Ç‡§∏ ‡§®‡•á ‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡•Ä ‡§∏‡§§‡§π ‡§™‡§∞ ‡§™‡§æ‡§®‡•Ä ‡§ï‡•á ‡§∏‡§Ç‡§ï‡•á‡§§ ‡§î‡§∞ ‡§ú‡•à‡§µ‡§ø‡§ï ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§ï‡•Ä ‡§ñ‡•ã‡§ú ‡§ï‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§Ø‡§π ‡§Ü‡§∂‡§Ç‡§ï‡§æ ‡§ú‡§§‡§æ‡§à ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§Ø‡§π‡§æ‡§Å ‡§ï‡§≠‡•Ä ‡§∏‡•Ç‡§ï‡•ç‡§∑‡•ç‡§Æ‡§ú‡•Ä‡§µ ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§•‡•á‡•§ ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§®‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ø‡§¶‡§ø ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞', tool_calls=None, raw=ModelResponse(id='5Rv89htHUXCZyawSsCMo24', created=1748849985, model='sutra-v2', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π, ‡§ú‡§ø‡§∏‡•á ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§Æ‡•á‡§Ç Mars ‡§ï‡§π‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§∏‡•å‡§∞ ‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡§æ ‡§ö‡•å‡§•‡§æ ‡§ó‡•ç‡§∞‡§π ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡•á ‡§≤‡§æ‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§ú‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§∞‡•ã‡§Æ‡§® ‡§Ø‡•Å‡§¶‡•ç‡§ß ‡§ï‡•á ‡§¶‡•á‡§µ‡§§‡§æ ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§™‡§∞ ‡§∞‡§ñ‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡§æ ‡§∞‡§Ç‡§ó ‡§≤‡§æ‡§≤ ‡§π‡•à, ‡§ú‡•ã ‡§ï‡§ø ‡§á‡§∏‡§ï‡•á ‡§∏‡§§‡§π ‡§™‡§∞ ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§Ü‡§Ø‡§∞‡§® ‡§ë‡§ï‡•ç‡§∏‡§æ‡§á‡§° (‡§ú‡§Ç‡§ó) ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ ‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§∏ ‡§≤‡§ó‡§≠‡§ó 6,779 ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§π‡•à, ‡§ú‡•ã ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡•á ‡§Ü‡§ß‡•á ‡§∏‡•á ‡§•‡•ã‡§°‡§º‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•à‡•§ ‡§Ø‡§π ‡§ó‡•ç‡§∞‡§π ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§∏‡•á ‡§§‡•Ä‡§∏‡§∞‡•Ä ‡§¶‡•Ç‡§∞‡•Ä ‡§™‡§∞ ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§ï‡•Ä ‡§ï‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ã ‡§™‡•Ç‡§∞‡§æ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§≤‡§ó‡§≠‡§ó 687 ‡§¶‡§ø‡§® ‡§≤‡§ó‡§§‡•á ‡§π‡•à‡§Ç‡•§\\n\\n‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•Ä ‡§∏‡§§‡§π ‡§™‡§∞ ‡§ï‡§à ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§è‡§Å ‡§π‡•à‡§Ç, ‡§ú‡•à‡§∏‡•á ‡§ï‡§ø ‡§ó‡§π‡§∞‡•Ä ‡§ò‡§æ‡§ü‡§ø‡§Ø‡§æ‡§Å, ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ‡§Æ‡•Å‡§ñ‡•Ä ‡§î‡§∞ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§Æ‡•à‡§¶‡§æ‡§®‡•Ä ‡§á‡§≤‡§æ‡§ï‡§º‡•á‡•§ ‡§á‡§∏‡§ï‡•Ä ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ ‡§ì‡§≤‡§Ç‡§™‡§∏ ‡§Æ‡•â‡§®‡•ç‡§∏ ‡§π‡•à, ‡§ú‡•ã ‡§∏‡•å‡§∞ ‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§¨‡§°‡§º‡§æ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ‡§Æ‡•Å‡§ñ‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§ï‡•Ä ‡§ä‡§Å‡§ö‡§æ‡§à ‡§≤‡§ó‡§≠‡§ó 22 ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ, ‡§µ‡•á‡§≤‡§ø‡§∏ ‡§Æ‡§æ‡§∞‡§ø‡§®‡•á‡§∞‡§ø‡§∏, ‡§ú‡•ã ‡§è‡§ï ‡§µ‡§ø‡§∂‡§æ‡§≤ ‡§ò‡§æ‡§ü‡•Ä ‡§π‡•à, ‡§≤‡§ó‡§≠‡§ó 4,000 ‡§ï‡§ø‡§≤‡•ã‡§Æ‡•Ä‡§ü‡§∞ ‡§≤‡§Ç‡§¨‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§Ø‡§π ‡§ß‡§∞‡§§‡•Ä ‡§ï‡•Ä ‡§ó‡•ç‡§∞‡•à‡§Ç‡§° ‡§ï‡•à‡§®‡•ç‡§Ø‡§® ‡§∏‡•á ‡§≠‡•Ä ‡§¨‡§°‡§º‡•Ä ‡§π‡•à‡•§ ‡§á‡§® ‡§≠‡•å‡§ó‡•ã‡§≤‡§ø‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§ì‡§Ç ‡§®‡•á ‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•ã ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§ï‡§æ ‡§è‡§ï ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§µ‡§ø‡§∑‡§Ø ‡§¨‡§®‡§æ ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à‡•§\\n\\n‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡•á ‡§µ‡§æ‡§Ø‡•Å‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡•Ä ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§™‡•É‡§•‡•ç‡§µ‡•Ä ‡§ï‡•Ä ‡§§‡•Å‡§≤‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§™‡§§‡§≤‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§§‡§É ‡§ï‡§æ‡§∞‡•ç‡§¨‡§® ‡§°‡§æ‡§á‡§ë‡§ï‡•ç‡§∏‡§æ‡§á‡§° (95.3%), ‡§®‡§æ‡§á‡§ü‡•ç‡§∞‡•ã‡§ú‡§® (2.7%) ‡§î‡§∞ ‡§Ü‡§∞‡•ç‡§ó‡§® (1.6%) ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à‡§Ç‡•§ ‡§µ‡§æ‡§Ø‡•Å‡§Æ‡§Ç‡§°‡§≤ ‡§ï‡•Ä ‡§á‡§∏ ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞ ‡§§‡§æ‡§™‡§Æ‡§æ‡§® ‡§¨‡§π‡•Å‡§§ ‡§ï‡§Æ ‡§∞‡§π‡§§‡§æ ‡§π‡•à, ‡§î‡§∏‡§§‡§® -80 ‡§°‡§ø‡§ó‡•ç‡§∞‡•Ä ‡§´‡§æ‡§∞‡•á‡§®‡§π‡§æ‡§á‡§ü (-62 ‡§°‡§ø‡§ó‡•ç‡§∞‡•Ä ‡§∏‡•á‡§≤‡•ç‡§∏‡§ø‡§Ø‡§∏) ‡§§‡§ï ‡§ó‡§ø‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡•á ‡§Ö‡§≤‡§æ‡§µ‡§æ, ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞ ‡§ß‡•Ç‡§≤ ‡§≠‡§∞‡•Ä ‡§Ü‡§Å‡§ß‡§ø‡§Ø‡§æ‡§Å ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç, ‡§ú‡•ã ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§™‡•Ç‡§∞‡•á ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§ï‡•ã ‡§¢‡§ï ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡§Ç‡•§\\n\\n‡§™‡§ø‡§õ‡§≤‡•á ‡§ï‡•Å‡§õ ‡§µ‡§∞‡•ç‡§∑‡•ã‡§Ç ‡§Æ‡•á‡§Ç, ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞ ‡§ú‡•Ä‡§µ‡§® ‡§ï‡•Ä ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ‡§ì‡§Ç ‡§ï‡•Ä ‡§ñ‡•ã‡§ú ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§à ‡§Æ‡§ø‡§∂‡§® ‡§≠‡•á‡§ú‡•á ‡§ó‡§è ‡§π‡•à‡§Ç‡•§ NASA ‡§ï‡•á ‡§∞‡•ã‡§µ‡§∞ ‡§ú‡•à‡§∏‡•á ‡§ï‡•ç‡§Ø‡•Ç‡§∞‡§ø‡§Ø‡•ã‡§∏‡§ø‡§ü‡•Ä ‡§î‡§∞ ‡§™‡§∞‡•ç‡§∏‡•á‡§µ‡•á‡§∞‡•á‡§Ç‡§∏ ‡§®‡•á ‡§Æ‡§Ç‡§ó‡§≤ ‡§ï‡•Ä ‡§∏‡§§‡§π ‡§™‡§∞ ‡§™‡§æ‡§®‡•Ä ‡§ï‡•á ‡§∏‡§Ç‡§ï‡•á‡§§ ‡§î‡§∞ ‡§ú‡•à‡§µ‡§ø‡§ï ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§ï‡•Ä ‡§ñ‡•ã‡§ú ‡§ï‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§Ø‡§π ‡§Ü‡§∂‡§Ç‡§ï‡§æ ‡§ú‡§§‡§æ‡§à ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§Ø‡§π‡§æ‡§Å ‡§ï‡§≠‡•Ä ‡§∏‡•Ç‡§ï‡•ç‡§∑‡•ç‡§Æ‡§ú‡•Ä‡§µ ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§•‡•á‡•§ ‡§µ‡•à‡§ú‡•ç‡§û‡§æ‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡§æ ‡§Æ‡§æ‡§®‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ø‡§¶‡§ø ‡§Æ‡§Ç‡§ó‡§≤ ‡§™‡§∞', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None), service_tier=None), token_usage=TokenUsage(input_tokens=0, output_tokens=0, total_tokens=0))\n"
          ]
        }
      ],
      "source": [
        "from smolagents import LiteLLMModel\n",
        "\n",
        "# Multilingual message (in Hindi)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§Ç‡§ó‡§≤ ‡§ó‡•ç‡§∞‡§π ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç 5 ‡§™‡•à‡§∞‡§æ‡§ó‡•ç‡§∞‡§æ‡§´ ‡§¶‡•Ä‡§ú‡§ø‡§è\"}]}\n",
        "]\n",
        "\n",
        "# Instantiate LiteLLMModel with Sutra model\n",
        "model = LiteLLMModel(\n",
        "    model_id=\"openai/sutra-v2\",                  # Use Sutra via LiteLLM\n",
        "    api_base=\"https://api.two.ai/v2\",            # Sutra API base from TwoAI\n",
        "    api_key=os.getenv(\"SUTRA_API_KEY\"),          # Pass API key\n",
        "    temperature=0.7,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "# Generate response\n",
        "response = model(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP7UJ1ZmxKkz"
      },
      "source": [
        "###Create a CodeAgent with the DuckDuckGo Search Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDG4nMSqmciF"
      },
      "outputs": [],
      "source": [
        "from smolagents import LiteLLMModel, CodeAgent, DuckDuckGoSearchTool\n",
        "\n",
        "# Initialize Sutra model\n",
        "model = LiteLLMModel(\n",
        "    model_id=\"openai/sutra-v2\",\n",
        "    api_base=\"https://api.two.ai/v2\",\n",
        "    api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "    temperature=0.7,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "# Initialize CodeAgent with DuckDuckGo search tool and Sutra model\n",
        "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)\n",
        "\n",
        "# Run a query using the agent\n",
        "agent.run(\"‡∞ê‡∞∏‡±Ä‡∞∏‡±Ä 2025 ‡∞´‡±à‡∞®‡∞≤‡±ç ‡∞é‡∞µ‡∞∞‡±Å ‡∞ó‡±Ü‡∞≤‡∞ø‡∞ö‡∞æ‡∞∞‡±Å?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EgyDDp9yvLB"
      },
      "source": [
        "###Text-to-Speech Response from Sutra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqVVPfC3ols7"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Import necessary modules\n",
        "from smolagents import CodeAgent, LiteLLMModel\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "# ‚úÖ Initialize Sutra Model\n",
        "model = LiteLLMModel(\n",
        "    model_id=\"openai/sutra-v2\",\n",
        "    api_base=\"https://api.two.ai/v2\",\n",
        "    api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "    temperature=0.7,\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "# ‚úÖ Initialize CodeAgent\n",
        "agent = CodeAgent(tools=[], model=model, add_base_tools=True)\n",
        "\n",
        "# ‚úÖ Run the agent\n",
        "response = agent.run(\"About TWO AI\")\n",
        "print(\"Agent Response:\", response)\n",
        "\n",
        "# ‚úÖ Convert response to speech using gTTS\n",
        "tts = gTTS(text=str(response))\n",
        "tts.save(\"response.mp3\")\n",
        "\n",
        "# ‚úÖ Play audio in notebook\n",
        "Audio(\"response.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS6adKCXy12D"
      },
      "source": [
        "###Using SmolAgents to perform RAG on URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vBidBqOq_bm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from smolagents import LiteLLMModel, Tool\n",
        "from smolagents.agents import CodeAgent\n",
        "\n",
        "\n",
        "# ‚úÖ Step 1: Load text content from a URL\n",
        "def load_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# ‚úÖ Example: Use your own URL\n",
        "url = \"https://arxiv.org/pdf/1706.03762\"  # Replace with your own\n",
        "page_content = load_text_from_url(url)\n",
        "\n",
        "source_docs = [Document(page_content=page_content, metadata={\"source\": url})]\n",
        "\n",
        "# ‚úÖ Step 2: Split text into chunks\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer,\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "print(\"Splitting document...\")\n",
        "docs_processed = []\n",
        "unique_texts = {}\n",
        "for doc in tqdm(source_docs):\n",
        "    new_docs = text_splitter.split_documents([doc])\n",
        "    for new_doc in new_docs:\n",
        "        if new_doc.page_content not in unique_texts:\n",
        "            unique_texts[new_doc.page_content] = True\n",
        "            docs_processed.append(new_doc)\n",
        "\n",
        "# ‚úÖ Step 3: Embed & store in vector DB\n",
        "print(\"Embedding documents...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vector_store = Chroma.from_documents(docs_processed, embeddings, persist_directory=\"./chroma_db\")\n",
        "\n",
        "\n",
        "# ‚úÖ Step 4: Create Retrieval Tool\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Retrieve the most relevant docs from URL-based knowledge base.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query for semantic search.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, vector_store, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str), \"Query must be a string\"\n",
        "        docs = self.vector_store.similarity_search(query, k=3)\n",
        "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [f\"\\n\\n===== Document {i} =====\\n\" + doc.page_content for i, doc in enumerate(docs)]\n",
        "        )\n",
        "\n",
        "\n",
        "retriever_tool = RetrieverTool(vector_store)\n",
        "\n",
        "# ‚úÖ Step 5: Setup Sutra LLM\n",
        "model = LiteLLMModel(\n",
        "    model_id=\"openai/sutra-v2\",\n",
        "    api_base=\"https://api.two.ai/v2\",\n",
        "    api_key=userdata.get(\"SUTRA_API_KEY\"),\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 6: Run Agent\n",
        "agent = CodeAgent(\n",
        "    tools=[retriever_tool],\n",
        "    model=model,\n",
        "    max_steps=4,\n",
        "    verbosity_level=2,\n",
        ")\n",
        "\n",
        "query = \"What are encoders and decoders\"\n",
        "agent_output = agent.run(query)\n",
        "\n",
        "print(\"Final output:\")\n",
        "print(agent_output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
