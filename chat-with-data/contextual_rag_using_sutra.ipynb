{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy2YO2HzQ5cM"
      },
      "source": [
        "<div style=\"display: flex; align-items: center; gap: 40px;\">\n",
        "\n",
        "<img src=\"https://play-lh.googleusercontent.com/_O9p4Z4yucA2NLmZBu9mTJCuBwXeT9NcbtrDN6I8gKlkIPRySV0adOmbyipjSj9Gew\" width=\"130\">\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "  <h2>SUTRA by TWO Platforms </h2>\n",
        "  <p>SUTRA is a family of large multi-lingual language (LMLMs) models pioneered by Two Platforms. SUTRAâ€™s dual-transformer approach extends the power of both MoE and Dense AI language model architectures, delivering cost-efficient multilingual capabilities for over 50+ languages. It powers scalable AI applications for conversation, search, and advanced reasoning, ensuring high-performance across diverse languages, domains and applications.</p>\n",
        "\n",
        "\n",
        "  <h2>Contextual RAG Using SUTRA</h2>\n",
        "  <p>Contextual Retrieval-Augmented Generation (RAG) is an advanced RAG technique that improves response relevance and efficiency by incorporating contextual compression during the retrieval process. Traditional RAG retrieves and sends full documents to the generation model, which may include irrelevant information, leading to higher costs and less accurate responses.\n",
        "</p>\n",
        "\n",
        "<p>In Contextual RAG, the retrieved documents are processed through a Document Compressor before being passed to the language model. This compressor extracts and retains only the most relevant information for the query, or even discards entire irrelevant documents. This approach reduces the noise in the retrieved context, resulting in more precise, concise, and cost-effective responses from the generation model..</p>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SGrkXLcXWdnUm8IQI-2Gp5pxT78oLSZK?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdcvY-IRTFl"
      },
      "source": [
        "## Get Your API Keys\n",
        "\n",
        "Before you begin, make sure you have:\n",
        "\n",
        "1. A SUTRA API key (Get yours at [TWO AI's SUTRA API page](https://www.two.ai/sutra/api))\n",
        "2. Basic familiarity with Python and Jupyter notebooks\n",
        "\n",
        "This notebook is designed to run in Google Colab, so no local Python installation is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_FcHDjKRXlK"
      },
      "source": [
        "###ðŸ”§ 1. Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "H0rhmYh2OmlK",
        "outputId": "cb7fece0-a1a7-47f4-a863-4041211984fb"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_openai langchain_community chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pdTd9EJRc9b"
      },
      "source": [
        "###ðŸ”‘ 2. Set Environment Variables (API Keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "383zgfHiOqJv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set the API key from Colab secrets\n",
        "os.environ[\"SUTRA_API_KEY\"] = userdata.get(\"SUTRA_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsEhMfAdR6Wa"
      },
      "source": [
        "###Initialize Sutra LLM for chat generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb7blo96O-Y_"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "import os\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=os.getenv(\"SUTRA_API_KEY\"),\n",
        "    base_url=\"https://api.two.ai/v2\",\n",
        "    model=\"sutra-v2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3OYjjgoR_jC"
      },
      "source": [
        "###Load & split documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MoLLgNwPAtf"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = CSVLoader(\"./context.csv\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "documents = text_splitter.split_documents(documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPe_mTLFSMDj"
      },
      "source": [
        "###Initialize Openai embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93I4PN54RwyK"
      },
      "outputs": [],
      "source": [
        "# load embedding model\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_bXyg5RSTzk"
      },
      "source": [
        "###Create vector store with Openai embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELyRPQM_PX-g"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95RgGn4LSbb7"
      },
      "source": [
        "###Setup Contextual Compression Retriever with Sutra LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reEaHey2Pnyk"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Bl9kV5ShEc"
      },
      "source": [
        "###Create prompt & RAG chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsgfEEuHPrNg"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "template = \"\"\"\n",
        "You are a helpful assistant that answers questions based on the following context.\n",
        "If you don't find the answer in the context, just say that you don't know.\n",
        "Context: {context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": compression_retriever, \"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt7K5Gx9TFW9"
      },
      "source": [
        "###Query example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voNfKy2uPtbA",
        "outputId": "653ffe21-d785-4166-f144-657e29232309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Points on a mortgage, also known as discount points, are a form of pre-paid interest that borrowers can pay to reduce the interest rate on their loan. One point equals one percent of the loan amount. By paying points, borrowers can lower their monthly payments in exchange for a higher upfront cost. Additionally, points can be used to help qualify for a loan by reducing the monthly payment, making it easier to meet income qualifications. Points differ from other fees such as origination fees or broker fees, as they specifically serve to buy down the interest rate.\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke(\"what are points on a mortgage\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
